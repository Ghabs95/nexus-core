"""
Shared agent launching logic for inbox processor and webhook server.

This module provides a unified interface for launching GitHub Copilot agents
in response to workflow events, whether triggered by polling (inbox processor)
or webhooks (webhook server).
"""

import asyncio
import glob
import logging
import os
import re
import subprocess
import threading
import time

from audit_store import AuditStore
from config import (
    BASE_DIR,
    ORCHESTRATOR_CONFIG,
    NEXUS_STORAGE_BACKEND,
    PROJECT_CONFIG,
    get_repo,
    get_repos,
    get_nexus_dir_name,
    get_project_platform,
    get_tasks_logs_dir,
)
from integrations.notifications import notify_agent_completed, emit_alert
from orchestration.ai_orchestrator import get_orchestrator
from orchestration.plugin_runtime import get_profiled_plugin
from state_manager import HostStateManager

# Nexus Core framework imports
from nexus.core.guards import LaunchGuard
from nexus.core.project.repo_utils import (
    iter_project_configs as _iter_project_configs,
)
from nexus.core.project.repo_utils import (
    project_repos_from_config as _project_repos,
)
from nexus.plugins.builtin.ai_runtime_plugin import ToolUnavailableError

logger = logging.getLogger(__name__)
_git_platform_cache = {}
_launch_policy_plugin = None


def _db_only_task_mode() -> bool:
    return str(NEXUS_STORAGE_BACKEND or "").strip().lower() == "postgres"


def _run_coro_sync(coro_factory):
    try:
        asyncio.get_running_loop()
        in_running_loop = True
    except RuntimeError:
        in_running_loop = False

    if not in_running_loop:
        try:
            return asyncio.run(coro_factory())
        except Exception:
            return None

    holder: dict[str, object | Exception | None] = {"value": None, "error": None}

    def _runner() -> None:
        try:
            holder["value"] = asyncio.run(coro_factory())
        except Exception as exc:
            holder["error"] = exc

    import threading

    t = threading.Thread(target=_runner, daemon=True)
    t.start()
    t.join(timeout=10)
    if t.is_alive() or holder.get("error") is not None:
        return None
    return holder.get("value")


_NON_AGENT_TRIGGER_SOURCES = {
    "unknown",
    "github_webhook",
    "push_completion",
    "pr_opened",
    "completion-scan",
    "orchestrator",
    "orphan-recovery",
    "dead-agent-retry",
    "orphan-timeout-retry",
    "timeout-retry",
}


def _completed_agent_from_trigger(trigger_source: str) -> str | None:
    """Return completed-agent label only when trigger_source is agent-like."""
    source = str(trigger_source or "").strip().lstrip("@").strip()
    if not source:
        return None
    if source.lower().startswith("manual-"):
        return "manual"
    normalized = source.lower()
    if normalized in _NON_AGENT_TRIGGER_SOURCES:
        return None
    if not re.fullmatch(r"[a-zA-Z][a-zA-Z0-9_-]{0,63}", source):
        return None
    return normalized


def _get_git_platform_client(repo: str, project_name: str | None = None):
    """Return cached abstract git platform adapter for repository."""
    from orchestration.nexus_core_helpers import get_git_platform

    cache_key = f"{project_name or ''}:{repo}"
    if cache_key in _git_platform_cache:
        return _git_platform_cache[cache_key]

    platform = get_git_platform(repo=repo, project_name=project_name or None)
    if platform:
        _git_platform_cache[cache_key] = platform
    return platform


def _resolve_project_from_task_file(task_file: str) -> str:
    """Resolve project key by matching task file path against project workspaces."""
    for project_key, project_cfg in _iter_project_configs(PROJECT_CONFIG, get_repos):
        workspace_abs = os.path.join(BASE_DIR, str(project_cfg["workspace"]))
        if task_file.startswith(workspace_abs):
            return project_key
    return ""


def _resolve_project_from_repo(repo_name: str) -> str:
    """Resolve project key from configured project repo mappings."""
    target = str(repo_name or "").strip()
    if not target:
        return ""
    for project_key, cfg in _iter_project_configs(PROJECT_CONFIG, get_repos):
        if target in _project_repos(project_key, cfg, get_repos):
            return project_key
    return ""


def _load_issue_body_from_project_repo(issue_number: str, preferred_repo: str | None = None):
    """Load issue body from the repo that matches task-file/project boundaries.

    Returns:
        ``(body, repo, task_file)`` when resolved, otherwise ``("", "", "")``.
    """
    issue_number = str(issue_number)
    candidate_repos: list[tuple[str, str]] = []
    for project_key, cfg in _iter_project_configs(PROJECT_CONFIG, get_repos):
        project_repos = _project_repos(project_key, cfg, get_repos)
        for repo_name in project_repos:
            pair = (project_key, repo_name)
            if pair not in candidate_repos:
                candidate_repos.append(pair)

    preferred = str(preferred_repo or "").strip()
    if preferred:
        preferred_pairs = [pair for pair in candidate_repos if pair[1] == preferred]
        other_pairs = [pair for pair in candidate_repos if pair[1] != preferred]
        candidate_repos = preferred_pairs + other_pairs

    for project_key, repo_name in candidate_repos:
        try:
            platform = _get_git_platform_client(repo_name, project_name=project_key)
        except Exception as exc:
            logger.warning(
                "Skipping issue probe for issue #%s in %s (%s): %s",
                issue_number,
                repo_name,
                project_key,
                exc,
            )
            continue
        if not platform:
            continue

        issue = _run_coro_sync(lambda: platform.get_issue(issue_number))
        if not issue:
            continue

        body = str(getattr(issue, "body", "") or "")
        if not body:
            continue

        task_file_match = re.search(r"\*\*Task File:\*\*\s*`([^`]+)`", body)
        task_file = task_file_match.group(1) if task_file_match else ""
        if _db_only_task_mode():
            # In DB-only mode, trust project->repo binding and avoid local task-file path resolution.
            return body, repo_name, task_file
        if not task_file:
            continue

        resolved_project_key = _resolve_project_from_task_file(task_file)
        if not resolved_project_key:
            continue

        project_cfg = PROJECT_CONFIG.get(resolved_project_key, {})
        expected_repos = _project_repos(resolved_project_key, project_cfg, get_repos)
        if repo_name not in expected_repos:
            continue

        return body, repo_name, task_file

    return "", "", ""


def _get_launch_policy_plugin():
    """Return shared agent launch policy plugin instance."""
    global _launch_policy_plugin
    if _launch_policy_plugin:
        return _launch_policy_plugin

    plugin = get_profiled_plugin(
        "agent_launch_policy",
        cache_key="agent-launch:policy",
    )
    if plugin:
        _launch_policy_plugin = plugin
    return plugin


def _merge_excluded_tools(*tool_lists) -> list[str]:
    """Merge tool names preserving first-seen order."""
    merged: list[str] = []
    for tools in tool_lists:
        if not tools:
            continue
        for tool in tools:
            value = str(tool or "").strip().lower()
            if value and value not in merged:
                merged.append(value)
    return merged


def _tool_is_rate_limited(orchestrator, tool_name: str) -> bool:
    """Return True when orchestrator has an active rate-limit cooldown for tool."""
    try:
        rate_limits = getattr(orchestrator, "_rate_limits", {})
        if not isinstance(rate_limits, dict):
            return False
        info = rate_limits.get(str(tool_name or "").strip().lower())
        if not isinstance(info, dict):
            return False
        until = float(info.get("until", 0) or 0)
        return until > time.time()
    except Exception:
        return False


def _persist_issue_excluded_tools(issue_num: str, tools: list[str]) -> None:
    """Persist issue-level excluded tools in launched_agents state."""
    if not issue_num or issue_num == "unknown":
        return
    merged = _merge_excluded_tools(tools)
    if not merged:
        return

    launched_agents = HostStateManager.load_launched_agents(recent_only=False)
    previous_entry = launched_agents.get(str(issue_num), {})
    if not isinstance(previous_entry, dict):
        previous_entry = {}

    entry = dict(previous_entry)
    entry["exclude_tools"] = _merge_excluded_tools(previous_entry.get("exclude_tools", []), merged)
    launched_agents[str(issue_num)] = entry
    HostStateManager.save_launched_agents(launched_agents)


def _read_tail(path: str, max_chars: int = 4000) -> str:
    try:
        with open(path, encoding="utf-8", errors="replace") as handle:
            data = handle.read()
        if len(data) <= max_chars:
            return data
        return data[-max_chars:]
    except Exception:
        return ""


def _log_indicates_copilot_quota_failure(log_text: str) -> bool:
    text = str(log_text or "").lower()
    if not text:
        return False
    quota_markers = (
        "402",
        "429",
        "quota",
        "rate limit",
        "ratelimit",
        "too many requests",
        "status: 429",
        "statustext: 'too many requests'",
    )
    summary_markers = ("total session time", "total usage est", "api time spent")
    return any(marker in text for marker in quota_markers) and any(
        marker in text for marker in summary_markers
    )


def _log_indicates_gemini_quota_failure(log_text: str) -> bool:
    text = str(log_text or "").lower()
    if not text:
        return False
    quota_markers = (
        "retryablequotaerror",
        "exhausted your capacity",
        "quota will reset",
        "status: 429",
        "status 429",
        "too many requests",
        "no capacity available",
    )
    loop_markers = ("retrying after", "attempt 1 failed", "attempt 2 failed")
    return any(marker in text for marker in quota_markers) and any(
        marker in text for marker in loop_markers
    )


def _log_indicates_codex_quota_failure(log_text: str) -> bool:
    text = str(log_text or "").lower()
    if not text:
        return False
    quota_markers = (
        "429",
        "too many requests",
        "rate limit",
        "ratelimit",
        "quota",
        "insufficient_quota",
        "retryablequotaerror",
        "exhausted your capacity",
    )
    loop_markers = ("retrying after", "attempt 1 failed", "attempt 2 failed", "retry")
    return any(marker in text for marker in quota_markers) and any(
        marker in text for marker in loop_markers
    )


def _find_latest_copilot_log(workspace_dir: str, project_key: str | None, issue_num: str) -> str:
    project = str(project_key or "nexus")
    log_dir = get_tasks_logs_dir(workspace_dir, project)
    pattern = os.path.join(log_dir, f"copilot_{issue_num}_*.log")
    candidates = glob.glob(pattern)
    if not candidates:
        return ""
    return max(candidates, key=os.path.getmtime)


def _find_latest_gemini_log(workspace_dir: str, project_key: str | None, issue_num: str) -> str:
    project = str(project_key or "nexus")
    log_dir = get_tasks_logs_dir(workspace_dir, project)
    pattern = os.path.join(log_dir, f"gemini_{issue_num}_*.log")
    candidates = glob.glob(pattern)
    if not candidates:
        return ""
    return max(candidates, key=os.path.getmtime)


def _find_latest_codex_log(workspace_dir: str, project_key: str | None, issue_num: str) -> str:
    project = str(project_key or "nexus")
    log_dir = get_tasks_logs_dir(workspace_dir, project)
    pattern = os.path.join(log_dir, f"codex_{issue_num}_*.log")
    candidates = glob.glob(pattern)
    if not candidates:
        return ""
    return max(candidates, key=os.path.getmtime)


def _start_copilot_post_exit_watchdog(
    *,
    issue_num: str,
    pid: int,
    agent_type: str,
    prompt: str,
    workspace_dir: str,
    agents_dir: str,
    base_dir: str,
    log_subdir: str | None,
    agent_env: dict[str, str] | None,
    orchestrator,
    exclude_tools: list[str] | None,
    tier_name: str,
    mode: str,
    workflow_name: str,
) -> None:
    def _worker() -> None:
        logger.info(
            "Copilot watchdog started for issue #%s (pid=%s, agent=%s)",
            issue_num,
            pid,
            agent_type,
        )
        deadline = time.time() + 300
        while time.time() < deadline:
            launched = HostStateManager.load_launched_agents(recent_only=False)
            current = launched.get(str(issue_num), {})
            if not isinstance(current, dict):
                return
            current_pid = int(current.get("pid", 0) or 0)
            current_tool = str(current.get("tool", "")).strip().lower()
            if current_pid != int(pid) or current_tool != "copilot":
                return

            log_path = _find_latest_copilot_log(workspace_dir, log_subdir, issue_num)
            if log_path:
                tail = _read_tail(log_path)
                if _log_indicates_copilot_quota_failure(tail):
                    logger.warning(
                        "Copilot watchdog detected quota/rate-limit for issue #%s (pid=%s).",
                        issue_num,
                        pid,
                    )
                    merged_exclusions = _merge_excluded_tools(exclude_tools or [], ["copilot"])
                    logger.info(
                        "Copilot watchdog fallback-started for issue #%s with exclusions=%s",
                        issue_num,
                        merged_exclusions,
                    )
                    _persist_issue_excluded_tools(issue_num, ["copilot"])
                    logger.warning(
                        "Copilot post-exit quota detected for issue #%s (pid=%s). Auto-fallback with exclusions=%s",
                        issue_num,
                        pid,
                        merged_exclusions,
                    )
                    emit_alert(
                        (
                            f"⚠️ Copilot quota detected after launch for issue #{issue_num}. "
                            "Auto-switching to fallback provider."
                        ),
                        severity="warning",
                        source="agent_launcher",
                        issue_number=str(issue_num),
                        project_key=str(log_subdir or "nexus"),
                    )
                    try:
                        logger.info(
                            "Copilot watchdog invoking fallback chain for issue #%s (exclude=%s)",
                            issue_num,
                            merged_exclusions,
                        )
                        pid_new, tool_new = orchestrator.invoke_agent(
                            agent_prompt=prompt,
                            workspace_dir=workspace_dir,
                            agents_dir=agents_dir,
                            base_dir=base_dir,
                            issue_url=f"https://github.com/{get_repo(str(log_subdir or 'nexus'))}/issues/{issue_num}",
                            agent_name=agent_type,
                            use_gemini=False,
                            exclude_tools=merged_exclusions,
                            log_subdir=log_subdir,
                            env=agent_env,
                        )
                        if pid_new:
                            new_tool = str(getattr(tool_new, "value", tool_new))
                            launched_agents = HostStateManager.load_launched_agents(
                                recent_only=False
                            )
                            prev = launched_agents.get(str(issue_num), {})
                            if not isinstance(prev, dict):
                                prev = {}
                            launched_agents[str(issue_num)] = {
                                **prev,
                                "timestamp": time.time(),
                                "pid": pid_new,
                                "tier": tier_name,
                                "mode": mode,
                                "tool": new_tool,
                                "agent_type": agent_type,
                                "exclude_tools": _merge_excluded_tools(
                                    prev.get("exclude_tools", []), merged_exclusions
                                ),
                            }
                            HostStateManager.save_launched_agents(launched_agents)
                            record_agent_launch(issue_num, agent_type=agent_type, pid=pid_new)
                            AuditStore.audit_log(
                                int(issue_num),
                                "AGENT_LAUNCHED",
                                f"Auto-fallback launched {new_tool} after Copilot quota (workflow: {workflow_name}/{tier_name}, mode: {mode}, PID: {pid_new})",
                            )
                            logger.info(
                                "✅ Auto-fallback launch succeeded for issue #%s with %s (PID: %s)",
                                issue_num,
                                new_tool,
                                pid_new,
                            )
                            _attach_post_launch_watchdog(
                                tool_name=new_tool,
                                issue_num=str(issue_num),
                                pid=int(pid_new),
                                agent_type=str(agent_type),
                                prompt=str(prompt),
                                workspace_dir=str(workspace_dir),
                                agents_dir=str(agents_dir),
                                base_dir=str(base_dir),
                                log_subdir=str(log_subdir or "nexus"),
                                agent_env=agent_env,
                                orchestrator=orchestrator,
                                exclude_tools=merged_exclusions,
                                tier_name=str(tier_name),
                                mode=str(mode),
                                workflow_name=str(workflow_name),
                            )
                        else:
                            logger.error(
                                "Auto-fallback launch returned no PID for issue #%s after Copilot quota",
                                issue_num,
                            )
                            emit_alert(
                                (
                                    f"❌ No AI providers available after Copilot quota on issue #{issue_num}. "
                                    "All fallback providers are unavailable or rate-limited."
                                ),
                                severity="error",
                                source="agent_launcher",
                                issue_number=str(issue_num),
                                project_key=str(log_subdir or "nexus"),
                            )
                            AuditStore.audit_log(
                                int(issue_num),
                                "AGENT_LAUNCH_FAILED",
                                "No fallback providers available after Copilot quota watchdog trigger",
                            )
                    except ToolUnavailableError as exc:
                        logger.error(
                            "Copilot watchdog fallback exhausted providers for issue #%s: %s",
                            issue_num,
                            exc,
                        )
                        emit_alert(
                            (
                                f"❌ No AI providers available for issue #{issue_num} "
                                f"after Copilot quota: {exc}"
                            ),
                            severity="error",
                            source="agent_launcher",
                            issue_number=str(issue_num),
                            project_key=str(log_subdir or "nexus"),
                        )
                        AuditStore.audit_log(
                            int(issue_num),
                            "AGENT_LAUNCH_FAILED",
                            f"Fallback exhausted after Copilot quota: {exc}",
                        )
                    except Exception as exc:
                        logger.error(
                            "Auto-fallback launch failed for issue #%s after Copilot quota: %s",
                            issue_num,
                            exc,
                            exc_info=True,
                        )
                    return

            # Stop early if process no longer exists and no quota markers were detected.
            try:
                os.kill(int(pid), 0)
            except Exception:
                return
            time.sleep(2)

    threading.Thread(
        target=_worker,
        name=f"copilot-watchdog-{issue_num}",
        daemon=True,
    ).start()


def _start_gemini_quota_watchdog(
    *,
    issue_num: str,
    pid: int,
    agent_type: str,
    prompt: str,
    workspace_dir: str,
    agents_dir: str,
    base_dir: str,
    log_subdir: str | None,
    agent_env: dict[str, str] | None,
    orchestrator,
    exclude_tools: list[str] | None,
    tier_name: str,
    mode: str,
    workflow_name: str,
) -> None:
    def _worker() -> None:
        def _on_quota_detected(*, reason: str, terminate_pid: bool) -> None:
            logger.warning(
                "Gemini watchdog detected quota/rate-limit for issue #%s (pid=%s, reason=%s).",
                issue_num,
                pid,
                reason,
            )
            if terminate_pid:
                try:
                    os.kill(int(pid), 15)
                    logger.info(
                        "Gemini watchdog terminated pid=%s for issue #%s",
                        pid,
                        issue_num,
                    )
                except Exception:
                    pass

            merged_exclusions = _merge_excluded_tools(exclude_tools or [], ["gemini"])
            _persist_issue_excluded_tools(issue_num, ["gemini"])
            logger.warning(
                "Gemini quota loop detected for issue #%s (pid=%s). Auto-fallback with exclusions=%s",
                issue_num,
                pid,
                merged_exclusions,
            )
            emit_alert(
                (
                    f"⚠️ Gemini quota detected after launch for issue #{issue_num}. "
                    "Auto-switching to fallback provider."
                ),
                severity="warning",
                source="agent_launcher",
                issue_number=str(issue_num),
                project_key=str(log_subdir or "nexus"),
            )
            try:
                logger.info(
                    "Gemini watchdog invoking fallback chain for issue #%s (exclude=%s)",
                    issue_num,
                    merged_exclusions,
                )
                pid_new, tool_new = orchestrator.invoke_agent(
                    agent_prompt=prompt,
                    workspace_dir=workspace_dir,
                    agents_dir=agents_dir,
                    base_dir=base_dir,
                    issue_url=f"https://github.com/{get_repo(str(log_subdir or 'nexus'))}/issues/{issue_num}",
                    agent_name=agent_type,
                    use_gemini=False,
                    exclude_tools=merged_exclusions,
                    log_subdir=log_subdir,
                    env=agent_env,
                )
                if pid_new:
                    new_tool = str(getattr(tool_new, "value", tool_new))
                    launched_agents = HostStateManager.load_launched_agents(recent_only=False)
                    prev = launched_agents.get(str(issue_num), {})
                    if not isinstance(prev, dict):
                        prev = {}
                    launched_agents[str(issue_num)] = {
                        **prev,
                        "timestamp": time.time(),
                        "pid": pid_new,
                        "tier": tier_name,
                        "mode": mode,
                        "tool": new_tool,
                        "agent_type": agent_type,
                        "exclude_tools": _merge_excluded_tools(
                            prev.get("exclude_tools", []), merged_exclusions
                        ),
                    }
                    HostStateManager.save_launched_agents(launched_agents)
                    record_agent_launch(issue_num, agent_type=agent_type, pid=pid_new)
                    AuditStore.audit_log(
                        int(issue_num),
                        "AGENT_LAUNCHED",
                        f"Auto-fallback launched {new_tool} after Gemini quota (workflow: {workflow_name}/{tier_name}, mode: {mode}, PID: {pid_new})",
                    )
                    logger.info(
                        "✅ Auto-fallback launch succeeded for issue #%s with %s (PID: %s)",
                        issue_num,
                        new_tool,
                        pid_new,
                    )
                    _attach_post_launch_watchdog(
                        tool_name=new_tool,
                        issue_num=str(issue_num),
                        pid=int(pid_new),
                        agent_type=str(agent_type),
                        prompt=str(prompt),
                        workspace_dir=str(workspace_dir),
                        agents_dir=str(agents_dir),
                        base_dir=str(base_dir),
                        log_subdir=str(log_subdir or "nexus"),
                        agent_env=agent_env,
                        orchestrator=orchestrator,
                        exclude_tools=merged_exclusions,
                        tier_name=str(tier_name),
                        mode=str(mode),
                        workflow_name=str(workflow_name),
                    )
                else:
                    logger.error(
                        "Auto-fallback launch returned no PID for issue #%s after Gemini quota",
                        issue_num,
                    )
                    emit_alert(
                        (
                            f"❌ No AI providers available after Gemini quota on issue #{issue_num}. "
                            "All fallback providers are unavailable or rate-limited."
                        ),
                        severity="error",
                        source="agent_launcher",
                        issue_number=str(issue_num),
                        project_key=str(log_subdir or "nexus"),
                    )
                    AuditStore.audit_log(
                        int(issue_num),
                        "AGENT_LAUNCH_FAILED",
                        "No fallback providers available after Gemini quota watchdog trigger",
                    )
            except ToolUnavailableError as exc:
                logger.error(
                    "Gemini watchdog fallback exhausted providers for issue #%s: %s",
                    issue_num,
                    exc,
                )
                emit_alert(
                    (
                        f"❌ No AI providers available for issue #{issue_num} "
                        f"after Gemini quota: {exc}"
                    ),
                    severity="error",
                    source="agent_launcher",
                    issue_number=str(issue_num),
                    project_key=str(log_subdir or "nexus"),
                )
                AuditStore.audit_log(
                    int(issue_num),
                    "AGENT_LAUNCH_FAILED",
                    f"Fallback exhausted after Gemini quota: {exc}",
                )
            except Exception as exc:
                logger.error(
                    "Auto-fallback launch failed for issue #%s after Gemini quota: %s",
                    issue_num,
                    exc,
                    exc_info=True,
                )

        logger.info(
            "Gemini watchdog started for issue #%s (pid=%s, agent=%s)",
            issue_num,
            pid,
            agent_type,
        )
        deadline = time.time() + 300
        while time.time() < deadline:
            launched = HostStateManager.load_launched_agents(recent_only=False)
            current = launched.get(str(issue_num), {})
            if not isinstance(current, dict):
                return
            current_pid = int(current.get("pid", 0) or 0)
            current_tool = str(current.get("tool", "")).strip().lower()
            if current_pid != int(pid) or current_tool != "gemini":
                return

            log_path = _find_latest_gemini_log(workspace_dir, log_subdir, issue_num)
            if log_path:
                tail = _read_tail(log_path)
                if _log_indicates_gemini_quota_failure(tail):
                    _on_quota_detected(reason="log-loop", terminate_pid=True)
                    return

            try:
                os.kill(int(pid), 0)
            except Exception:
                if log_path:
                    tail = _read_tail(log_path)
                    if _log_indicates_gemini_quota_failure(tail):
                        _on_quota_detected(reason="post-exit-log", terminate_pid=False)
                return
            time.sleep(2)

    threading.Thread(
        target=_worker,
        name=f"gemini-watchdog-{issue_num}",
        daemon=True,
    ).start()


def _start_codex_quota_watchdog(
    *,
    issue_num: str,
    pid: int,
    agent_type: str,
    prompt: str,
    workspace_dir: str,
    agents_dir: str,
    base_dir: str,
    log_subdir: str | None,
    agent_env: dict[str, str] | None,
    orchestrator,
    exclude_tools: list[str] | None,
    tier_name: str,
    mode: str,
    workflow_name: str,
) -> None:
    def _worker() -> None:
        logger.info(
            "Codex watchdog started for issue #%s (pid=%s, agent=%s)",
            issue_num,
            pid,
            agent_type,
        )
        deadline = time.time() + 300
        while time.time() < deadline:
            launched = HostStateManager.load_launched_agents(recent_only=False)
            current = launched.get(str(issue_num), {})
            if not isinstance(current, dict):
                return
            current_pid = int(current.get("pid", 0) or 0)
            current_tool = str(current.get("tool", "")).strip().lower()
            if current_pid != int(pid) or current_tool != "codex":
                return

            log_path = _find_latest_codex_log(workspace_dir, log_subdir, issue_num)
            if log_path:
                tail = _read_tail(log_path)
                if _log_indicates_codex_quota_failure(tail):
                    logger.warning(
                        "Codex watchdog detected quota/rate-limit for issue #%s (pid=%s).",
                        issue_num,
                        pid,
                    )
                    try:
                        os.kill(int(pid), 15)
                        logger.info(
                            "Codex watchdog terminated pid=%s for issue #%s",
                            pid,
                            issue_num,
                        )
                    except Exception:
                        pass
                    merged_exclusions = _merge_excluded_tools(exclude_tools or [], ["codex"])
                    _persist_issue_excluded_tools(issue_num, ["codex"])
                    logger.warning(
                        "Codex quota loop detected for issue #%s (pid=%s). Auto-fallback with exclusions=%s",
                        issue_num,
                        pid,
                        merged_exclusions,
                    )
                    emit_alert(
                        (
                            f"⚠️ Codex quota detected after launch for issue #{issue_num}. "
                            "Auto-switching to fallback provider."
                        ),
                        severity="warning",
                        source="agent_launcher",
                        issue_number=str(issue_num),
                        project_key=str(log_subdir or "nexus"),
                    )
                    try:
                        logger.info(
                            "Codex watchdog invoking fallback chain for issue #%s (exclude=%s)",
                            issue_num,
                            merged_exclusions,
                        )
                        pid_new, tool_new = orchestrator.invoke_agent(
                            agent_prompt=prompt,
                            workspace_dir=workspace_dir,
                            agents_dir=agents_dir,
                            base_dir=base_dir,
                            issue_url=f"https://github.com/{get_repo(str(log_subdir or 'nexus'))}/issues/{issue_num}",
                            agent_name=agent_type,
                            use_gemini=False,
                            exclude_tools=merged_exclusions,
                            log_subdir=log_subdir,
                            env=agent_env,
                        )
                        if pid_new:
                            new_tool = str(getattr(tool_new, "value", tool_new))
                            launched_agents = HostStateManager.load_launched_agents(
                                recent_only=False
                            )
                            prev = launched_agents.get(str(issue_num), {})
                            if not isinstance(prev, dict):
                                prev = {}
                            launched_agents[str(issue_num)] = {
                                **prev,
                                "timestamp": time.time(),
                                "pid": pid_new,
                                "tier": tier_name,
                                "mode": mode,
                                "tool": new_tool,
                                "agent_type": agent_type,
                                "exclude_tools": _merge_excluded_tools(
                                    prev.get("exclude_tools", []), merged_exclusions
                                ),
                            }
                            HostStateManager.save_launched_agents(launched_agents)
                            record_agent_launch(issue_num, agent_type=agent_type, pid=pid_new)
                            AuditStore.audit_log(
                                int(issue_num),
                                "AGENT_LAUNCHED",
                                f"Auto-fallback launched {new_tool} after Codex quota (workflow: {workflow_name}/{tier_name}, mode: {mode}, PID: {pid_new})",
                            )
                            logger.info(
                                "✅ Auto-fallback launch succeeded for issue #%s with %s (PID: %s)",
                                issue_num,
                                new_tool,
                                pid_new,
                            )
                        else:
                            logger.error(
                                "Auto-fallback launch returned no PID for issue #%s after Codex quota",
                                issue_num,
                            )
                            emit_alert(
                                (
                                    f"❌ No AI providers available after Codex quota on issue #{issue_num}. "
                                    "All fallback providers are unavailable or rate-limited."
                                ),
                                severity="error",
                                source="agent_launcher",
                                issue_number=str(issue_num),
                                project_key=str(log_subdir or "nexus"),
                            )
                            AuditStore.audit_log(
                                int(issue_num),
                                "AGENT_LAUNCH_FAILED",
                                "No fallback providers available after Codex quota watchdog trigger",
                            )
                    except ToolUnavailableError as exc:
                        logger.error(
                            "Codex watchdog fallback exhausted providers for issue #%s: %s",
                            issue_num,
                            exc,
                        )
                        emit_alert(
                            (
                                f"❌ No AI providers available for issue #{issue_num} "
                                f"after Codex quota: {exc}"
                            ),
                            severity="error",
                            source="agent_launcher",
                            issue_number=str(issue_num),
                            project_key=str(log_subdir or "nexus"),
                        )
                        AuditStore.audit_log(
                            int(issue_num),
                            "AGENT_LAUNCH_FAILED",
                            f"Fallback exhausted after Codex quota: {exc}",
                        )
                    except Exception as exc:
                        logger.error(
                            "Auto-fallback launch failed for issue #%s after Codex quota: %s",
                            issue_num,
                            exc,
                            exc_info=True,
                        )
                    return

            try:
                os.kill(int(pid), 0)
            except Exception:
                return
            time.sleep(2)

    threading.Thread(
        target=_worker,
        name=f"codex-watchdog-{issue_num}",
        daemon=True,
    ).start()


def _attach_post_launch_watchdog(
    *,
    tool_name: str,
    issue_num: str,
    pid: int,
    agent_type: str,
    prompt: str,
    workspace_dir: str,
    agents_dir: str,
    base_dir: str,
    log_subdir: str | None,
    agent_env: dict[str, str] | None,
    orchestrator,
    exclude_tools: list[str] | None,
    tier_name: str,
    mode: str,
    workflow_name: str,
) -> None:
    normalized_tool = str(tool_name or "").strip().lower()
    if normalized_tool == "copilot":
        _start_copilot_post_exit_watchdog(
            issue_num=issue_num,
            pid=pid,
            agent_type=agent_type,
            prompt=prompt,
            workspace_dir=workspace_dir,
            agents_dir=agents_dir,
            base_dir=base_dir,
            log_subdir=log_subdir,
            agent_env=agent_env,
            orchestrator=orchestrator,
            exclude_tools=exclude_tools,
            tier_name=tier_name,
            mode=mode,
            workflow_name=workflow_name,
        )
    elif normalized_tool == "gemini":
        _start_gemini_quota_watchdog(
            issue_num=issue_num,
            pid=pid,
            agent_type=agent_type,
            prompt=prompt,
            workspace_dir=workspace_dir,
            agents_dir=agents_dir,
            base_dir=base_dir,
            log_subdir=log_subdir,
            agent_env=agent_env,
            orchestrator=orchestrator,
            exclude_tools=exclude_tools,
            tier_name=tier_name,
            mode=mode,
            workflow_name=workflow_name,
        )
    elif normalized_tool == "codex":
        _start_codex_quota_watchdog(
            issue_num=issue_num,
            pid=pid,
            agent_type=agent_type,
            prompt=prompt,
            workspace_dir=workspace_dir,
            agents_dir=agents_dir,
            base_dir=base_dir,
            log_subdir=log_subdir,
            agent_env=agent_env,
            orchestrator=orchestrator,
            exclude_tools=exclude_tools,
            tier_name=tier_name,
            mode=mode,
            workflow_name=workflow_name,
        )


def _pgrep_and_logfile_guard(issue_id: str, agent_type: str) -> bool:
    """Custom guard: returns True (allow) if no running process AND no recent log.

    Check 1: pgrep for running Copilot process on this issue
    Check 2: recent log files (within last 2 minutes)
    """
    # Check 1: Running processes
    try:
        check_result = subprocess.run(
            ["pgrep", "-af", f"copilot.*issues/{issue_id}[^0-9]|copilot.*issues/{issue_id}$"],
            text=True,
            capture_output=True,
            timeout=5,
        )
        if check_result.stdout:
            logger.info(f"⏭️ Agent already running for issue #{issue_id} (PID found)")
            return False
    except Exception:
        pass

    # Check 2: Recent log files (within last 2 minutes)
    nexus_dir_name = get_nexus_dir_name()
    recent_logs = glob.glob(
        os.path.join(
            BASE_DIR,
            "**",
            nexus_dir_name,
            "tasks",
            "logs",
            "**",
            f"copilot_{issue_id}_*.log",
        ),
        recursive=True,
    )
    if recent_logs:
        recent_logs.sort(key=lambda p: os.path.getmtime(p), reverse=True)
        latest_log_age = time.time() - os.path.getmtime(recent_logs[0])
        if latest_log_age < 120:
            logger.info(f"⏭️ Recent log file for issue #{issue_id} ({latest_log_age:.0f}s old)")
            return False

    return True  # allow launch


# Module-level singleton — LaunchGuard with 120s cooldown + pgrep/logfile custom guard.
_launch_guard = LaunchGuard(
    cooldown_seconds=120,
    custom_guard=_pgrep_and_logfile_guard,
)


def is_recent_launch(issue_number: str, agent_type: str = "*") -> bool:
    """Check if an agent was recently launched for this issue.

    Delegates to nexus-core's LaunchGuard (cooldown + pgrep + logfile checks).
    Returns True if launched within cooldown window.
    """
    normalized_agent = str(agent_type or "*").strip() or "*"
    return not _launch_guard.can_launch(str(issue_number), agent_type=normalized_agent)


def record_agent_launch(issue_number: str, agent_type: str = "*", pid: int = None) -> None:
    """Record a successful agent launch in the LaunchGuard."""
    normalized_agent = str(agent_type or "*").strip() or "*"
    _launch_guard.record_launch(str(issue_number), agent_type=normalized_agent, pid=pid)


def clear_launch_guard(issue_number: str) -> int:
    """Clear the LaunchGuard for an issue, allowing an immediate relaunch.

    Used by the dead-agent retry path to bypass the cooldown window when
    we intentionally want to relaunch a crashed agent.

    Returns:
        Number of cleared records.
    """
    return _launch_guard.clear(str(issue_number))


def _resolve_workflow_path(project_name: str = None) -> str:
    """Resolve workflow definition path for project or global config."""
    workflow_path = ""
    if project_name:
        project_cfg = PROJECT_CONFIG.get(project_name, {})
        if isinstance(project_cfg, dict):
            workflow_path = project_cfg.get("workflow_definition_path", "")

    if not workflow_path:
        workflow_path = PROJECT_CONFIG.get("workflow_definition_path", "")

    if workflow_path and not os.path.isabs(workflow_path):
        workflow_path = os.path.join(BASE_DIR, workflow_path)

    return workflow_path


def _is_git_repo(path: str) -> bool:
    if not path or not os.path.isdir(path):
        return False
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--is-inside-work-tree"],
            cwd=path,
            check=False,
            capture_output=True,
            text=True,
        )
        return result.returncode == 0
    except Exception:
        return False


def _resolve_worktree_base_repo(workspace_dir: str, issue_url: str) -> str:
    """Resolve git repo root used for worktree provisioning.

    For mono-workspace setups where ``workspace_dir`` is a parent folder (e.g. ``/home/.../ghabs``),
    use repo name from issue URL to find nested checkout (e.g. ``/home/.../ghabs/nexus-core``).
    """
    base = str(workspace_dir or "").strip()
    if _is_git_repo(base):
        return base

    match = re.search(r"https?://[^/]+/[^/]+/([^/]+)/issues/\d+", str(issue_url or ""))
    repo_name = match.group(1).strip() if match else ""
    if repo_name:
        nested = os.path.join(base, repo_name)
        if _is_git_repo(nested):
            return nested

    return base


def _build_agent_search_dirs(agents_dir: str) -> list:
    """Build the ordered list of directories to search for agent YAML files.

    Starts with the project-specific *agents_dir*, then appends the shared
    org-level agents directory configured via ``shared_agents_dir`` in config.
    """
    dirs = [agents_dir]
    shared = PROJECT_CONFIG.get("shared_agents_dir", "")
    if shared:
        shared_abs = os.path.join(BASE_DIR, shared) if not os.path.isabs(shared) else shared
        if shared_abs != agents_dir:
            dirs.append(shared_abs)
    return dirs


from nexus.core.execution import ExecutionEngine, find_agent_definition


def _resolve_skill_name(agent_type: str) -> str:
    """Normalize agent name for workspace skill directory."""
    return re.sub(r"[^a-z0-9]+", "_", agent_type.lower()).strip("_")


def _ensure_agent_definition(
    agents_dir: str, agent_type: str, workspace_dir: str | None = None
) -> bool:
    """Ensure an agent definition exists and sync workspace skills if needed."""
    search_dirs = _build_agent_search_dirs(agents_dir)
    yaml_path = find_agent_definition(agent_type, search_dirs)

    if not yaml_path:
        msg = f"Missing agent YAML for agent_type '{agent_type}' in {search_dirs}"
        logger.error(msg)
        emit_alert(msg, severity="error", source="agent_launcher")
        return False

    agent_md_path = os.path.splitext(yaml_path)[0] + ".agent.md"

    # Generate instructions if missing or outdated
    needs_sync = False
    if not os.path.exists(agent_md_path) or os.path.getmtime(agent_md_path) < os.path.getmtime(
        yaml_path
    ):
        try:
            from nexus.translators.to_copilot import translate_agent_to_copilot

            md_content = translate_agent_to_copilot(yaml_path)
            if md_content:
                with open(agent_md_path, "w", encoding="utf-8") as handle:
                    handle.write(md_content)
                logger.info(f"✅ Generated agent instructions: {agent_md_path}")
                needs_sync = True
            else:
                return False
        except Exception as e:
            logger.error(f"Translator error for {yaml_path}: {e}")
            return False
    else:
        needs_sync = True

    # Sync to workspace skill
    if needs_sync and workspace_dir:
        try:
            with open(agent_md_path, encoding="utf-8") as f:
                content = f.read()
            ExecutionEngine.sync_workspace_skill(workspace_dir, agent_type, content)
        except Exception as e:
            logger.warning(f"Failed to sync workspace skill for {agent_type}: {e}")

    return True


def get_sop_tier_from_issue(issue_number, project="nexus", repo_override: str | None = None):
    """Get workflow tier from issue labels.

    Delegates to nexus-core's GitPlatform.get_workflow_type_from_issue().

    Args:
        issue_number: Git issue number
        project: Project name to determine repo

    Returns: tier_name (full/shortened/fast-track) or None
    """
    from nexus.adapters.git.github import GitHubPlatform

    from orchestration.nexus_core_helpers import get_git_platform

    repo = str(repo_override or "")
    try:
        repo = repo or get_repo(project)
        platform_type = get_project_platform(project)

        if platform_type == "github":
            platform = GitHubPlatform(repo)
            return platform.get_workflow_type_from_issue(int(issue_number))

        issue = asyncio.run(
            get_git_platform(repo, project_name=project).get_issue(str(issue_number))
        )
        if not issue:
            return None
        labels = {str(label).lower() for label in (issue.labels or [])}
        if "workflow:fast-track" in labels:
            return "fast-track"
        if "workflow:shortened" in labels:
            return "shortened"
        if "workflow:full" in labels:
            return "full"
        return None
    except Exception as e:
        logger.error(f"Failed to get tier from issue #{issue_number} in {project} ({repo}): {e}")
        return None


def get_workflow_name(tier_name):
    """Returns the workflow slash-command name for the tier."""
    policy = _get_launch_policy_plugin()
    if policy and hasattr(policy, "get_workflow_name"):
        return policy.get_workflow_name(tier_name)
    if tier_name in {"fast-track", "shortened"}:
        return "bug_fix"
    return "new_feature"


def invoke_copilot_agent(
    agents_dir,
    workspace_dir,
    issue_url,
    tier_name,
    task_content,
    continuation=False,
    continuation_prompt=None,
    use_gemini=False,
    exclude_tools=None,
    log_subdir=None,
    agent_type="triage",
    project_name=None,
):
    """Invokes an AI agent on the agents directory to process the task.

    Uses orchestrator to determine best tool (Copilot or Gemini CLI) with fallback support.
    Runs asynchronously (Popen) since agent execution can take several minutes.

    Args:
        agents_dir: Path to agents directory
        workspace_dir: Path to workspace directory
        issue_url: Git issue URL
        tier_name: Workflow tier (full/shortened/fast-track)
        task_content: Task description
        continuation: If True, this is a continuation of previous work
        continuation_prompt: Custom prompt for continuation
        use_gemini: If True, prefer Gemini CLI; if False, prefer Copilot (default: False)
        agent_type: Agent type to route to (triage, design, analysis, etc.)
        project_name: Project name for resolving workflow definition

    Returns:
        Tuple of (PID of launched process or None if failed, tool_used: str)
    """
    pid = None
    tool_name: str | None = None
    workflow_name = get_workflow_name(tier_name)
    workflow_path = _resolve_workflow_path(project_name)
    policy = _get_launch_policy_plugin()
    if policy and hasattr(policy, "build_agent_prompt"):
        prompt = policy.build_agent_prompt(
            issue_url=issue_url,
            tier_name=tier_name,
            task_content=task_content,
            agent_type=agent_type,
            continuation=continuation,
            continuation_prompt=continuation_prompt,
            workflow_path=workflow_path,
            nexus_dir=get_nexus_dir_name(),
            project_name=project_name,
        )
    else:
        prompt = (
            f"You are a {agent_type} agent.\n\n"
            f"Issue: {issue_url}\n"
            f"Tier: {tier_name}\n"
            f"Workflow: /{workflow_name}\n\n"
            f"Task details:\n{task_content}"
        )

    mode = "continuation" if continuation else "initial"
    logger.info(f"🤖 Launching {agent_type} agent in {agents_dir} (mode: {mode})")
    logger.info(f"   Workspace: {workspace_dir}")
    logger.info(f"   Workflow: /{workflow_name} (tier: {tier_name})")

    if not _ensure_agent_definition(agents_dir, agent_type, workspace_dir):
        return None, None

    # Use orchestrator to launch agent
    orchestrator = get_orchestrator(ORCHESTRATOR_CONFIG)

    # Resolve project-specific API token
    from orchestration.nexus_core_helpers import _get_project_config

    project_cfg = _get_project_config().get(project_name, {}) if project_name else {}
    git_platform = project_cfg.get("git_platform", "github")
    default_token_var = "GITLAB_TOKEN" if git_platform == "gitlab" else "GITHUB_TOKEN"
    token_var = project_cfg.get("git_token_var_name", default_token_var)
    token = os.getenv(token_var)

    agent_env = None
    if token:
        agent_env = {"GITHUB_TOKEN": token, "GITLAB_TOKEN": token}

    try:
        from nexus.core.workspace import WorkspaceManager

        # Extract issue number for tracking
        issue_match = re.search(r"/issues/(\d+)", issue_url or "")
        issue_num = issue_match.group(1) if issue_match else "unknown"

        worktree_base_repo = _resolve_worktree_base_repo(workspace_dir, issue_url)
        isolated_workspace = worktree_base_repo

        # Extract branch name from issue body if available
        target_branch = None
        if issue_url and issue_num != "unknown":
            try:
                repo_match = re.search(
                    r"https?://[^/]+/([^/]+/[^/]+)/issues/\d+",
                    str(issue_url or ""),
                )
                preferred_repo = repo_match.group(1) if repo_match else None
                body, _, _ = _load_issue_body_from_project_repo(
                    issue_num,
                    preferred_repo=preferred_repo,
                )
                if body:
                    branch_match = re.search(r"Target Branch:\s*`([^`]+)`", body)
                    if branch_match:
                        target_branch = branch_match.group(1)
                        logger.info(f"Using target branch from issue: {target_branch}")
            except Exception as e:
                logger.warning(f"Could not extract target branch: {e}")

        if issue_num != "unknown" and _is_git_repo(worktree_base_repo):
            isolated_workspace = WorkspaceManager.provision_worktree(
                worktree_base_repo, issue_num, branch_name=target_branch
            )
        elif issue_num != "unknown":
            logger.warning(
                "Skipping worktree provisioning for issue %s: not a git repo (%s)",
                issue_num,
                worktree_base_repo,
            )

        pid, tool_used = orchestrator.invoke_agent(
            agent_prompt=prompt,
            workspace_dir=isolated_workspace,
            agents_dir=agents_dir,
            base_dir=BASE_DIR,
            issue_url=issue_url,
            agent_name=agent_type,
            use_gemini=use_gemini,
            exclude_tools=exclude_tools,
            log_subdir=log_subdir,
            env=agent_env,
        )

        tool_name = tool_used.value
        logger.info(f"🚀 Agent launched with {tool_name} (PID: {pid})")

        # Save to launched agents tracker and emit audit, but never fail launch
        # response if bookkeeping has an internal error.
        if issue_num != "unknown":
            try:
                dynamic_exclusions = []
                if _tool_is_rate_limited(orchestrator, "gemini"):
                    dynamic_exclusions.append("gemini")

                launched_agents = HostStateManager.load_launched_agents()
                previous_entry = launched_agents.get(str(issue_num), {})
                if not isinstance(previous_entry, dict):
                    previous_entry = {}

                entry = dict(previous_entry)
                entry.update(
                    {
                        "timestamp": time.time(),
                        "pid": pid,
                        "tier": tier_name,
                        "mode": mode,
                        "tool": tool_name,
                        "agent_type": agent_type,
                        "exclude_tools": _merge_excluded_tools(
                            previous_entry.get("exclude_tools", []),
                            list(exclude_tools) if exclude_tools else [],
                            dynamic_exclusions,
                        ),
                    }
                )
                launched_agents[str(issue_num)] = entry
                HostStateManager.save_launched_agents(launched_agents)

                record_agent_launch(issue_num, agent_type=agent_type, pid=pid)

                AuditStore.audit_log(
                    int(issue_num),
                    "AGENT_LAUNCHED",
                    f"Launched {tool_name} agent in {os.path.basename(agents_dir)} "
                    f"(workflow: {workflow_name}/{tier_name}, mode: {mode}, PID: {pid})",
                )

                _attach_post_launch_watchdog(
                    tool_name=str(tool_name),
                    issue_num=str(issue_num),
                    pid=int(pid),
                    agent_type=str(agent_type),
                    prompt=str(prompt),
                    workspace_dir=str(isolated_workspace),
                    agents_dir=str(agents_dir),
                    base_dir=str(BASE_DIR),
                    log_subdir=str(log_subdir or project_name or "nexus"),
                    agent_env=agent_env,
                    orchestrator=orchestrator,
                    exclude_tools=list(exclude_tools) if exclude_tools else [],
                    tier_name=str(tier_name),
                    mode=str(mode),
                    workflow_name=str(workflow_name),
                )
            except Exception as bookkeeping_exc:
                logger.warning(
                    "Agent launch bookkeeping failed for issue #%s after successful launch: %s",
                    issue_num,
                    bookkeeping_exc,
                )

        return pid, tool_name

    except ToolUnavailableError as e:
        logger.error(f"❌ All AI tools unavailable: {e}")

        issue_match = re.search(r"/issues/(\d+)", issue_url or "")
        issue_num = issue_match.group(1) if issue_match else "unknown"
        message = str(e).lower()
        if (
            "gemini(rate-limited)" in message
            or "no capacity available" in message
            or _tool_is_rate_limited(orchestrator, "gemini")
        ):
            _persist_issue_excluded_tools(issue_num, ["gemini"])
            logger.info(
                "Persisted issue-level exclusion for Gemini on issue #%s due to rate-limit failure",
                issue_num,
            )
        if issue_num != "unknown":
            AuditStore.audit_log(
                int(issue_num), "AGENT_LAUNCH_FAILED", f"All tools unavailable: {str(e)}"
            )

        return None, None
    except Exception as e:
        logger.error(f"❌ Failed to launch agent: {e}")

        if pid and tool_name:
            logger.warning(
                "Returning successful launch despite post-launch exception (PID: %s, tool: %s)",
                pid,
                tool_name,
            )
            return pid, tool_name

        issue_match = re.search(r"/issues/(\d+)", issue_url or "")
        issue_num = issue_match.group(1) if issue_match else "unknown"
        if issue_num != "unknown":
            AuditStore.audit_log(int(issue_num), "AGENT_LAUNCH_FAILED", f"Exception: {str(e)}")

        return None, None


def launch_next_agent(
    issue_number,
    next_agent,
    trigger_source="unknown",
    exclude_tools=None,
    repo_override: str | None = None,
):
    """
    Launch the next agent in the workflow chain.

    This is the main entry point used by both inbox_processor and webhook_server.

    Args:
        issue_number: Git issue number (string or int)
        next_agent: Name of the agent to launch (e.g., "Atlas", "Architect")
        trigger_source: Where the trigger came from ("github_webhook", "log_file", "github_comment")
        exclude_tools: List of tool names to exclude from this launch attempt.
        repo_override: Preferred repository full_name for resolving issue metadata.

    Returns:
        ``(pid, tool_name)`` on success.
        ``(None, "duplicate-suppressed")`` when a duplicate launch is intentionally skipped.
        ``(None, None)`` on failure.
    """
    issue_number = str(issue_number)
    logger.info(
        f"🔗 Launching next agent @{next_agent} for issue #{issue_number} (trigger: {trigger_source})"
    )

    # Check for duplicate launches
    if is_recent_launch(issue_number, next_agent):
        logger.info(f"⏭️ Skipping duplicate launch for issue #{issue_number} agent @{next_agent}")
        return None, "duplicate-suppressed"

    # Get issue details from the repo matching this issue's task-file project
    try:
        body, resolved_repo, resolved_task_file = _load_issue_body_from_project_repo(
            issue_number,
            preferred_repo=repo_override,
        )
        if not body:
            logger.error(
                f"Failed to resolve issue #{issue_number} from configured project repositories"
            )
            return None, None
    except Exception as e:
        logger.error(f"Failed to resolve/process issue #{issue_number} body: {e}")
        return None, None

    # Find task-file metadata (optional in postgres mode)
    task_file_match = re.search(r"\*\*Task File:\*\*\s*`([^`]+)`", body)
    task_file = task_file_match.group(1) if task_file_match else (resolved_task_file or "")
    if not task_file and not _db_only_task_mode():
        logger.warning(f"No task file in issue #{issue_number}")
        return None, None

    if task_file and resolved_task_file and resolved_task_file != task_file:
        logger.warning(
            f"Issue #{issue_number} task file mismatch between resolution and body parse: "
            f"{resolved_task_file} vs {task_file}"
        )
    normalized_task_file = task_file.replace("\\", "/")
    is_shared_active_task_file = "/active/" in normalized_task_file
    task_file_exists = False if _db_only_task_mode() else os.path.exists(task_file)
    if not _db_only_task_mode() and not task_file_exists and not is_shared_active_task_file:
        logger.warning(f"Task file not found: {task_file}")
        return None, None

    # Get project config
    project_root = ""
    config = {}
    if _db_only_task_mode():
        project_root = _resolve_project_from_repo(resolved_repo or "")
        config = PROJECT_CONFIG.get(project_root, {}) if project_root else {}
    else:
        for key, cfg in PROJECT_CONFIG.items():
            if not isinstance(cfg, dict):
                continue
            workspace = cfg.get("workspace")
            if workspace and task_file:
                workspace_abs = os.path.join(BASE_DIR, str(workspace))
                if task_file.startswith(workspace_abs):
                    project_root = key
                    config = cfg
                    break

    if not project_root or not config.get("agents_dir"):
        logger.warning(
            f"No project config for issue #{issue_number} (repo={resolved_repo or 'unknown'}, task_file={task_file or 'n/a'})"
        )
        return None, None

    expected_repos = _project_repos(project_root, config, get_repos)
    if resolved_repo and expected_repos and resolved_repo not in expected_repos:
        logger.error(
            f"Project boundary violation for issue #{issue_number}: "
            f"resolved repo {resolved_repo}, project repos {expected_repos}"
        )
        return None, None

    # Read task content
    # Use issue body as authoritative task snapshot when Task File points to the shared
    # active path, which can be overwritten by later issues and cause cross-issue bleed.
    task_content = body
    if _db_only_task_mode():
        logger.info(
            "Postgres mode: using issue body task snapshot for issue #%s (task-file metadata: %s)",
            issue_number,
            (task_file or "missing"),
        )
    elif is_shared_active_task_file:
        logger.info(
            "Using issue body task snapshot for issue #%s (shared active task file: %s)",
            issue_number,
            task_file,
        )
    elif task_file_exists:
        try:
            with open(task_file) as f:
                task_content = f.read()
        except Exception as e:
            logger.warning(
                "Failed to read task file %s for issue #%s, falling back to issue body: %s",
                task_file,
                issue_number,
                e,
            )

    # Get workflow tier: launched_agents tracker → issue labels → halt if unknown
    from state_manager import HostStateManager

    repo = resolved_repo or get_repo(project_root)
    tracker_tier = HostStateManager.get_last_tier_for_issue(issue_number)
    label_tier = get_sop_tier_from_issue(issue_number, project_root, repo_override=repo)
    tier_name = label_tier or tracker_tier
    if not tier_name:
        logger.error(
            f"Cannot determine workflow tier for issue #{issue_number}: "
            "no tracker entry and no workflow: label."
        )
        return None, None

    # Merge caller-provided exclude_tools with any persisted ones from previous runs
    if exclude_tools is None:
        launched_agents = HostStateManager.load_launched_agents()
        persisted = launched_agents.get(str(issue_number), {}).get("exclude_tools", [])
        if persisted:
            exclude_tools = list(persisted)
            logger.info(
                f"Restored persisted exclude_tools for issue #{issue_number}: {exclude_tools}"
            )

    issue_url = f"https://github.com/{repo}/issues/{issue_number}"
    agents_abs = os.path.join(BASE_DIR, config["agents_dir"])
    workspace_abs = os.path.join(BASE_DIR, config["workspace"])

    # Create continuation prompt
    continuation_prompt = (
        f"You are a {next_agent} agent. The previous workflow step is complete.\n\n"
        f"Your task: Begin your step in the workflow.\n"
        f"Read recent Git comments to understand what's been completed.\n"
        f"Then perform your assigned work and post a status update.\n"
        f"End with a completion marker like: 'Ready for `@NextAgent`'"
    )

    # Launch agent
    pid, tool_used = invoke_copilot_agent(
        agents_dir=agents_abs,
        workspace_dir=workspace_abs,
        issue_url=issue_url,
        tier_name=tier_name,
        task_content=task_content,
        continuation=True,
        continuation_prompt=continuation_prompt,
        exclude_tools=exclude_tools,
        log_subdir=project_root,
        agent_type=next_agent,
        project_name=project_root,
    )

    if pid:
        logger.info(
            f"✅ Successfully launched @{next_agent} for issue #{issue_number} "
            f"(PID: {pid}, tool: {tool_used})"
        )
        completed_agent = _completed_agent_from_trigger(trigger_source)
        if completed_agent:
            try:
                notify_agent_completed(
                    issue_number=str(issue_number),
                    completed_agent=completed_agent,
                    next_agent=next_agent,
                    project=project_root,
                )
            except Exception as e:
                logger.warning(f"Failed to send notification: {e}")
        return pid, tool_used
    else:
        logger.error(f"❌ Failed to launch @{next_agent} for issue #{issue_number}")
        return None, None
